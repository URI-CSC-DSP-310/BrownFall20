---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.12
    jupytext_version: 1.6.0
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Class 18: Mid Semester Checkin, Git, & How GNB makes decisions

- Share your favorite thing about fall (or just say hi) in the zoom chat for attendance
- log onto Prismia
- accept Assignment 6
- install git or GitBash if needed

```{code-cell} ipython3
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
sns.set_theme(font_scale = 2)
```


## Feedback Review

Thank you for the feedback. You can add more on Friday's notes if you did not give any yet.

```{code-cell} ipython3
feedback_df = pd.read_csv('data/feedback_structured.csv')
feedback_df.head()
```

```{code-cell} ipython3
el_meaning = {1: 'much less',
 2: 'a little less',
 3: 'about right',
 4: 'a little more ',
 5: 'much more'}

expected_learning = feedback_df[feedback_df.columns[0]]
el_counts,_ = np.histogram(expected_learning,bins = [i+.5 for i in range(6)])
el_df = pd.DataFrame(data = el_counts,index = el_meaning.values(),columns= [expected_learning._name],)
# el_df.reset_index(inplace=True)
# el_df = el_df.rename(columns= {'index':'amount'}).set_index('amount')
el_df.rename_axis(index='amount relative to expectations',inplace=True)
el_df.plot.bar(legend=False,title=expected_learning._name,);
```

```{code-cell} ipython3
u_meaning = {0:'None', 1:'A little',3:'Half', 4:'Almost all',5:'All'}
u_col = feedback_df[feedback_df.columns[1]]
u_counts,_ = np.histogram(u_col,bins = [i+.5 for i in range(6)])
understanding_df = pd.DataFrame(data = u_counts,index = u_meaning.values(),columns= [u_col._name],)

understanding_df.rename_axis(index='amount of material',inplace=True)
understanding_df.plot.bar(legend=False,title=u_col._name,);
```

```{code-cell} ipython3
align = [c_name for c_name in feedback_df.columns if 'align' in c_name][0]
align_df = feedback_df[align].copy()
align_counts = align_df.value_counts()
align_kw = {'well':'well aligned','more':'I know more','less':'I know less'}
# # kw_replace =
align_counts.rename(index={s: [align_kw[kw] for kw in align_kw.keys() if kw in s][0] for s in align_counts.index},inplace=True)
align_counts.plot.bar(title=align_counts._name);
```

```{code-cell} ipython3
reflect_cols = [c_name for c_name in feedback_df.columns if 'reflects' in c_name]
reflect_df = feedback_df[reflect_cols].copy()
reflect_df.rename(columns = lambda c: c.split('[')[1].replace(']',''),inplace=True)
reflect_df = reflect_df.melt(var_name='factor',value_name='grading reflection')
reflect_df['grading reflection'] = reflect_df['grading reflection'].str.replace(' in the grading','')
reflect_df['grading reflection'] = reflect_df['grading reflection'].str.replace('Reflected','')

factor_kw = ['follow instructions','understand','effort']
# kw_replace =
reflect_df.replace({s: [kw for kw in factor_kw if kw in s][0] for s in reflect_df['factor']},inplace=True)
g = sns.displot(data = reflect_df, x='grading reflection',col='factor', hue = 'grading reflection',)
g.set(xticklabels=[]);
```

```{code-cell} ipython3
fair_cols = [c_name for c_name in feedback_df.columns if 'fair' in c_name]
fair_df = feedback_df[fair_cols].copy()
fair_df.rename(columns = lambda c: c.split('[')[1].replace(']',''),inplace=True)
fair_df = fair_df.melt(var_name='factor',value_name='fairness')
fair_df['fairness'] = fair_df['fairness'].str.replace(' in the grading','')
# reflect_df['grading reflection'] = reflect_df['grading reflection'].str.replace('Reflected','')

factor_kw = ['follow instructions','understand','effort']
# # kw_replace =
fair_df.replace({s: [kw for kw in factor_kw if kw in s][0] for s in fair_df['factor']},inplace=True)
g = sns.displot(data = reflect_df, x='grading reflection',col='factor', hue = 'grading reflection',)
g.set(xticklabels=[]);
# fair_df
```

```{code-cell} ipython3
activity_col = [cname for cname in feedback_df.columns if 'outside' in cname][0]
activity_counts = pd.get_dummies(feedback_df[activity_col].str.split(',').apply(pd.Series).stack()).sum()
activity_counts = activity_counts.rename_axis(index='activites outside of class')
# activity_counts.plot.bar()
ac_clean = activity_counts.reset_index().rename(columns={0:'students'})
sns.barplot(data=ac_clean,y='activites outside of class',x='students',);
```


## Changes based on your feedback for the rest of the semester


- Assignments posted by Wed, due Tuesday (after this one, but I'm giving class time for it)
- Assignments will have a bit more detail or advice (this one is very structured because of the short time, but not all will be this closed).
- Office hours will stay the same, but deadlines moved
- Will collect descriptions of any [issued with Prismia.chat](https://forms.gle/L1n6iSTtUZe6mMeBA) to pass on to the developers

+++


## Assignment

- Accept
- Git demo
- Read & make notes of a plan (what do you need to do? what methods will you use?)
- Discuss your plan in a breakout room


Questions:
1. Errors means misclassifications, samples where the classifier's prediction is not correct.

+++


## More on Naive Bayes

From last week:
```{code-cell} ipython3
# %load http://drsmb.co/310
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
```

```{code-cell} ipython3
iris = sns.load_dataset("iris")

X_train, X_test, y_train, y_test = train_test_split(iris.values[:,:4],
                                                    iris.values[:,-1],
                                                    test_size=0.5, random_state=5)


gnb = GaussianNB()
y_pred = gnb.fit(X_train, y_train).predict(X_test)
```

Remember we can check accuracy with the score method
```{code-cell} ipython3
gnb.score(X_test,y_test)
```

We can inspect the trained classifier

```{code-cell} ipython3
gnb.__dict__
```

Gaussian Niave Bayes learns the mean and variance for each feature in each class.  We can use that to generate synthetic data.  

```{code-cell} ipython3
df = pd.DataFrame(np.concatenate([np.random.multivariate_normal(mu, sig*np.eye(4),20)
                                  for mu, sig in zip(gnb.theta_,gnb.sigma_)]))
df['species'] = [ci for cl in [[c]*20 for c in gnb.classes_] for ci in cl]
sns.pairplot(data =df, hue='species')
```

It decides for it's predictions by computing probabilities and predicting the highest probability one.  We can have it return these probabilities directly.

```{code-cell} ipython3
gnb.predict_proba(X_test)`
```

We can use these to see how confident it is on each point.  First, we'll get the probability of the predicted class (the max in each row of the matrix).

```{code-cell} ipython3
pob_ypred = np.max(gnb.predict_proba(X_test),axis=1)
pob_ypred
```

Now, we'll put the test data into a `DataFrame` so we can use high level plotting functions from `pandas` and `seaborn`.  We could use the `numpy` data structures that scikit learn uses directly with `matplotlib` plotting and `numpy` statistics, but this is a little more compact and readable.

```{code-cell} ipython3
iris_test_df = pd.DataFrame(X_test,columns = iris.columns[:4])
iris_test_df['species'] = y_test
iris_test_df['species_pred'] = y_pred
iris_test_df['pob_ypred'] = pob_ypred
iris_test_df.head()
```

First we'll look at the one pairwise view of the data that was the most visually separate, `'petal_width'` vs `'petal_length'`

```{code-cell} ipython3
iris_test_df.plot.scatter(x='petal_width', y = 'petal_length',c='pob_ypred')
```

Notice that the points near the boundary are the lowest probability. We can inspect further by adding a `'correct'` column with boolean values

```{code-cell} ipython3
iris_test_df['correct'] = iris_test_df['species'] == iris_test_df['species_pred']
```

Now we can use EDA (exploratory data analysis) to further examine.

```{code-cell} ipython3
iris_test_df.groupby('correct')['pob_ypred'].describe()
```

The incorrectly (`False`) predicted samples were much lower confidence and higher variance in confidence. This suggests that the classifier is a good fit for the data, though it's imperfect on the test set, and that we should trust how it's working.

## Try it yourself

1. Can any of these things help with Assignment 6?
1. Review the datasets you've used for prior analyses, can they be used for any classification?
1. What kinds of data would be needed to train a classifier for some ML systems that you interact with?
