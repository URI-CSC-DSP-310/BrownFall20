---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.12
    jupytext_version: 1.6.0
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Class 21: Regression

1. Log onto prismia
1. Share a topic you're most interested in applying data science to in the zoom chat

## Regression Introduction


What is the difference in data that's well suited for regression vs classification
- [ ] regression is better for more features
- [ ] regression can work with categorical features
- [x] regression uses a continuous target variable

Explanation: the difference is that for regression we can use continuous target variables.  Either classification or regression can work with high dimensional data (dimension refers to the number of features). Either can also work with a mixture of categorical or continuous valued features.




```{code-cell} ipython3
import numpy as np
import seaborn as sns
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
sns.set_theme(font_scale=2)
```



Today we will work with the [diabetes dataset](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset) and we'll use the `sklearn.datasets` module to load it.  When we load data this way, it gets loaded as a `numpy` array

```{code-cell} ipython3
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y = True)
```


Since it's not a `pandas.DataFrame` we don't have the head method, but we can index using square brackets.
```{code-cell} ipython3
diabetes_X[:5]
```

and the same for the labels.
```{code-cell} ipython3
diabetes_y[:5]
```

Since this data is not in any sorted order, we can split into test and train using indexing instead of the `test_train_split` function.  This way we all get the same split, without setting the `ranomd_seed` parameter.  In most cases, it's best to use the function, but it's good to know different ways to do things.

We'll start by using only one feature, the one in column 8.
```{code-cell} ipython3
diabetes_X_train = diabetes_X[:-20,8]
diabetes_X_test = diabetes_X[-20:,8]
diabetes_y_train = diabetes_y[:-20]
diabetes_y_test = diabetes_y[-20:]
```
Now we can instantiate the object

```{code-cell} ipython3
regr = linear_model.LinearRegression()
```

All `sklearn` estimators have the same methods, and all take any specialized parameters in the constructor.  We've used the default values here, but this is an important design feature of scikit learn, because it makes their pipeline infrastructure and functions for cross validation work.  

```{code-cell} ipython3
regr.fit(diabetes_X_train,diabetes_y_train)
```

we get an error because the training data  (`diabetes_X_train`) is only one dimensional.
Scikit learn's `fit` methods require a 2d-array even if fitting with only one feature.  


```{code-cell} ipython3
diabetes_X_train.shape
```

we can use `np.newaxis` to fix this problem.

```{code-cell} ipython3
diabetes_X_train = diabetes_X[:-20,np.newaxis,8]
diabetes_X_test = diabetes_X[-20:,np.newaxis,8]
```

and check the shape

```{code-cell} ipython3
diabetes_X[:-20,8].shape
```

It's the same shape, in a mathematical sense, but in terms of the underlying data structure this one has a second dimension (it's just only 1 long) in that dimension.  This is tricky, but an important thing to know how to fix.

Now we can fit our model

```{code-cell} ipython3
regr.fit(diabetes_X_train,diabetes_y_train)
```

and examine the coefficient

```{code-cell} ipython3
regr.coef_
```

and make predictions with the model

```{code-cell} ipython3
diabetes_y_pred = regr.predict(diabetes_X_test)
```

we can evaluate with `mean_squared_error` (see below for more detail), which is as it's named. It computes the error in each prediction (`y_pred- y_true`), squares it, then averages them all together.

```{code-cell} ipython3
mean_squared_error(diabetes_y_test,diabetes_y_pred)
```

We can also use the $r^2$ score, which is more normalized, it's best value is 1.

```{code-cell} ipython3
r2_score(diabetes_y_test,diabetes_y_pred)
```

To get an even better idea, we can plot.  We'll use `matplotlib` to plot, since our data is not in a DataFrame. `seaborn` and `pandas` use `matplotlib` under the hood, so it will have a lot of familiar methods, but it takes a lot more work to build complicated figures with `matplotlib`. The alias `plt` is used by convention for the `pyplot` module of `matplotlib`

```{code-cell} ipython3
import matplotlib.pyplot as plt
```

Then we can scatter plot the true data and make a line of the predictions in blue.

```{code-cell} ipython3
plt.scatter(diabetes_X_test,diabetes_y_test, color='black')
plt.plot(diabetes_X_test,diabetes_y_pred, color='blue')
```

```{admonition} Try it yourself

Fit another regression model to this data that uses all of the feautures instead of only one
```


```{code-cell} ipython3
diabetes_X_train2 = diabetes_X[:-20]
diabetes_X_test2 = diabetes_X[-20:]
diabetes_y_train2 = diabetes_y[:-20]
diabetes_y_test2 = diabetes_y[-20:]

regr2 = linear_model.LinearRegression()
regr2.fit(diabetes_X_train2,diabetes_y_train2)
diabetes_y_pred2 = regr2.predict(diabetes_X_test2)
r2_score(diabetes_y_test2,diabetes_y_pred2)
```

This does better on $r^2$, let's also check the mse.

```{code-cell} ipython3
mean_squared_error(diabetes_y_test2,diabetes_y_pred2)
```

## More practice

1. try out `regr2.score` what does scikite learn use for the score for `linear_model.LinearRegression` models?
1. repeat the above using `test_train_split`, can you make the test size 20 samples again?
1. try another single feature model using a different feature than we did above. Is your new feature a better or worse predictor? Make plots to compare and try to build intuition for the performance metrics geometrically.



## Questions After class

### What is considered a good r2 value?

This will depend on context in some sense. You can read about it in the [`r2_score` docs]() and the corresponding [wikipedia page](https://en.wikipedia.org/wiki/Coefficient_of_determination)

Here's a quick visual of what different r2 score values lookl like. I simulated data by ranomly picking 10 points, then made the "predicted" y values by picking a slope of 3 and computing `3*x`.  Then I simulated various levels of noise, by sampling noise and multiplying the same noise vector by different scales and adding all of those to a data frame with the column name the r score for if that column of target values was the truth.

Then I added some columns of y values that were with different slopes and different functions of x.  These all have the small amount of noise.


I used the pandas `melt` method to restructure the DataFrame so that I could use `FaceGrid` with `col` and `col_wrap` to show all of the results.  

```{code-cell} ipython3

x = 10*np.random.random(20)
y_pred = 3*x
ex_df = pd.DataFrame(data = x,columns = ['x'])
ex_df['y_pred'] = y_pred
n_levels = range(1,18,2)
noise = (np.random.random(20)-.5)*2
for n in n_levels:
    y_true = y_pred + n* noise
    ex_df['r2 = '+ str(np.round(r2_score(y_pred,y_true),3))] = y_true

f_x_list = [2*x,3.5*x,.5*x**2, .03*x**3, 10*np.sin(x)+x*3,3*np.log(x**2)]
for fx in f_x_list:
    y_true = fx + noise
    ex_df['r2 = '+ str(np.round(r2_score(y_pred,y_true),3))] = y_true    

xy_df = ex_df.melt(id_vars=['x','y_pred'],var_name='rscore',value_name='y')
# sns.lmplot(x='x',y='y', data = xy_df,col='rscore',col_wrap=3,)
g = sns.FacetGrid(data = xy_df,col='rscore',col_wrap=3,aspect=1.5,height=3)
g.map(plt.plot, 'x','y_pred',color='k')
g.map(sns.scatterplot, "x", "y",)
```

The `r2_score` alone doesn't tell the whole picture. We'll also often want to analyze the errors in greater detail.  We'll look at how to do that to investigate how the model really fits more later this week.

### Can some of the methods applied in regression can be used in the same format in classification?

the `sklearn` estimator objects all have the same methods, you can see details on the API on the [sklearn developer page](https://scikit-learn.org/stable/developers/develop.html).


### why couldn't we use train_test_split for this classifier?

We absolutely can. Splitting this way was to show *another* way to split the data, and point out under which conditions this way is ok.

### I am still confused about machine learning

Try reading the notes from the [introduction class](2020-10-12).

The scikit-learn [choose an estimator](https://scikit-learn.org/stable/_static/ml_map.png) graph can also be helpful.



### What is the difference between test and train data?

We split data, sample-wise, into two pieces, in order to evaluate what we are doing.

There is nothing inherently different between test data and train data.  In fact, by applying the model we learned with the training data to predict on the test data we are assuming that they are very similar.   

We split our data so that we can see how well we expect our trained model to work on new data, since the goal is to send our learned model into the world and use it for something else.  For example, a model fit on today's data, about diabetes, might be used to predict which patients are going to have the most severe disease in a year based on their stats today and then enroll them in a program to help them learn to better manage their diabetes.

The test set is meant to simulate future data that we've never seen before, but it's usually data *we* have seen, but our learning algorithm did not use to fit the model.


### What does cross validation mean?

Cross validation will come up again later, but basically cross validation is a more elaborate evaluation.  It repeatedly splits the data into test and train sets, fits the model, scores the model, and then returns all fo the results.


### What does `mean_squared_error` do?

It is a popular performance metric for regression.  To understand it, let's look back at what regression is doing.  Regression is tyring to predict a continuous valued quantity, so unlike in classification where we can just check if it matches or not, to compute performance, it makes sense consider the _size_ of the erorr, beacuse it's unlikely to get _exactly_ the right value for any prediction.


So, we might be tempted to compute error like this:

```{code-cell} ipython3
diabetes_y_test2-diabetes_y_pred2
```

this, however is a whole vector of quantities, so we would need to take the average.  Taking the average here though, will be very small, because some are positive and some are negative and they will cancel one another out.

```{code-cell} ipython3
np.mean(diabetes_y_test2-diabetes_y_pred2)

```

In this case, it turns out to even be negative.

Instead, we can square each error, to make them all postive.  We *could* also take the absolute value, but in a lot of calculations, we want to take the derivative, so suqaring instead of absolute value became more popular.  


```{code-cell} ipython3
(diabetes_y_test2-diabetes_y_pred2)**2

```

now we can take the mean of these to get a total average error.



```{code-cell} ipython3
mse_manual = np.mean((diabetes_y_test2-diabetes_y_pred2)**2)
mse_manual
```

we can confirm this gets the same answer as the sklearn function

```{code-cell} ipython3
mse_sk = mean_squared_error(diabetes_y_test2,diabetes_y_pred2)
mse_sk
```

We can even assert that they're the same, they're always going to be exactly the same


```{code-cell} ipython3
assert mse_manual == mse_sk
```
