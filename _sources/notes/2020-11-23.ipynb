{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 32: Intro to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. say hello on zoom\n",
    "1. share a sentence on the doc linked on prismia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load http://drsmb.co/310\n",
    "def classification_confint(acc, n):\n",
    "    '''\n",
    "    Compute the 95% confidence interval for a classification problem.\n",
    "      acc -- classification accuracy\n",
    "      n   -- number of observations used to compute the accuracy\n",
    "    Returns a tuple (lb,ub)\n",
    "    '''\n",
    "    interval = 1.96*np.sqrt(acc*(1-acc)/n)\n",
    "    lb = max(0, acc - interval)\n",
    "    ub = min(1.0, acc + interval)\n",
    "    return (lb,ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained to classifiers on the same data and evaluated on 50 test samples,\n",
    "to get accuracies of 78% and 90% is the difference significant?\n",
    "\n",
    "To check, we compute the confidence interval for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6651767828355258, 0.8948232171644742)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_confint(.78,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.816844242532462, 0.983155757467538)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_confint(.9,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we check to see if the intervals overlap.  They do, so these are not significantly\n",
    "different.\n",
    "\n",
    "This means that while those seem meaningfully different, with 50 samples, 78% vs 50%\n",
    "is not statistically significantly different. This means that we can't formally\n",
    "guarantee that the two classifiers have reliablly different perforamnce.\n",
    "\n",
    "If we had more samples, it could be, for example, for 200 samples we see that\n",
    "they are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8584221212662311, 0.941577878733769)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N =200\n",
    "classification_confint(.9,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.722588391417763, 0.8374116085822371)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_confint(.78,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "\n",
    "The first thing we need to do to be able to do to model text is transform to a\n",
    "numerical representation.  We can't use any of the models we've seen so far,\n",
    "or other models, on non numerical data.  \n",
    "\n",
    "terms:\n",
    "\n",
    "- document: unit of text we're analyzing (one sample)\n",
    "- token:  sequence of characters in some particular document that are grouped together as a useful semantic unit for processing (basically a word)\n",
    "- stop words: no meaning, we don't need them (like a, the, an,). Note that this is context dependent. [more info](https://scikit-learn.org/stable/modules/feature_extraction.html#using-stop-words)\n",
    "\n",
    "\n",
    "### Representation\n",
    "\n",
    "vector or bag of words, implemented by the `CountVectorizer`\n",
    "\n",
    "Some sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load http://drsmb.co/310\n",
    "text = {\n",
    "'Demeus Alves':'Hope everybody is staying safe',\n",
    "'Ryan Booth':'The power is out where I live, might be forced to leave soon',\n",
    "'Brianna MacDonald':'Rainy days',\n",
    "'Jair Delgado':'Can not wait for lunch... hungry',\n",
    "'Shawn Vincent':'I am excited for Thanksgiving',\n",
    "'Jacob Afonso':'Short weeks are the best!',\n",
    "'Ryan Buquicchio':'The sentence is sentence. (Best sentence ever)',\n",
    "'Nick McCaffery':'Very windy today',\n",
    "'David Perrone':'this is a sentence',\n",
    "'Masoud':'It is rainy here. What about there?',\n",
    "'Rony Lopes':'I get to relax later this week',\n",
    "'Patrick Dowd':'It is cold out today',\n",
    "'Ruifang Kuang':'Happy Thanksgiving!',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hope everybody is staying safe'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = text['Demeus Alves']\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we initalize the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can fit and transform at once, this will build the representation and return\n",
    "the input represented that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.fit_transform([s1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tells us the size and that it's a \"sparse matrix\" but that doesnt' display much more\n",
    "To see more we can cast it to a regular array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.fit_transform([s1]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't tell us much because this is all ones.\n",
    "\n",
    "Or look at the \"vocabulary\" also called the \"dictionary\" for the whole representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hope': 1, 'everybody': 0, 'is': 2, 'staying': 4, 'safe': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instead apply to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13x48 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 65 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.fit_transform(text.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are more rows (samples/documents) and more columns (words in vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hope': 16,\n",
       " 'everybody': 9,\n",
       " 'is': 18,\n",
       " 'staying': 34,\n",
       " 'safe': 30,\n",
       " 'the': 36,\n",
       " 'power': 27,\n",
       " 'out': 26,\n",
       " 'where': 46,\n",
       " 'live': 22,\n",
       " 'might': 24,\n",
       " 'be': 3,\n",
       " 'forced': 12,\n",
       " 'to': 39,\n",
       " 'leave': 21,\n",
       " 'soon': 33,\n",
       " 'rainy': 28,\n",
       " 'days': 7,\n",
       " 'can': 5,\n",
       " 'not': 25,\n",
       " 'wait': 42,\n",
       " 'for': 11,\n",
       " 'lunch': 23,\n",
       " 'hungry': 17,\n",
       " 'am': 1,\n",
       " 'excited': 10,\n",
       " 'thanksgiving': 35,\n",
       " 'short': 32,\n",
       " 'weeks': 44,\n",
       " 'are': 2,\n",
       " 'best': 4,\n",
       " 'sentence': 31,\n",
       " 'ever': 8,\n",
       " 'very': 41,\n",
       " 'windy': 47,\n",
       " 'today': 40,\n",
       " 'this': 38,\n",
       " 'it': 19,\n",
       " 'here': 15,\n",
       " 'what': 45,\n",
       " 'about': 0,\n",
       " 'there': 37,\n",
       " 'get': 13,\n",
       " 'relax': 29,\n",
       " 'later': 20,\n",
       " 'week': 43,\n",
       " 'cold': 6,\n",
       " 'happy': 14}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the transformed data to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = counts.fit_transform(text.values()).toarray()\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to read, we can use a dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index is the keys of the dictionary of the sentences. The columns are the\n",
    "words from the vocabulary. The  `get_feature_names` method will return them as a\n",
    "sorted list instead of a dictionary with numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>am</th>\n",
       "      <th>are</th>\n",
       "      <th>be</th>\n",
       "      <th>best</th>\n",
       "      <th>can</th>\n",
       "      <th>cold</th>\n",
       "      <th>days</th>\n",
       "      <th>ever</th>\n",
       "      <th>everybody</th>\n",
       "      <th>...</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>today</th>\n",
       "      <th>very</th>\n",
       "      <th>wait</th>\n",
       "      <th>week</th>\n",
       "      <th>weeks</th>\n",
       "      <th>what</th>\n",
       "      <th>where</th>\n",
       "      <th>windy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Demeus Alves</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ryan Booth</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brianna MacDonald</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jair Delgado</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shawn Vincent</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacob Afonso</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ryan Buquicchio</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nick McCaffery</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>David Perrone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Masoud</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rony Lopes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patrick Dowd</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ruifang Kuang</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   about  am  are  be  best  can  cold  days  ever  everybody  \\\n",
       "Demeus Alves           0   0    0   0     0    0     0     0     0          1   \n",
       "Ryan Booth             0   0    0   1     0    0     0     0     0          0   \n",
       "Brianna MacDonald      0   0    0   0     0    0     0     1     0          0   \n",
       "Jair Delgado           0   0    0   0     0    1     0     0     0          0   \n",
       "Shawn Vincent          0   1    0   0     0    0     0     0     0          0   \n",
       "Jacob Afonso           0   0    1   0     1    0     0     0     0          0   \n",
       "Ryan Buquicchio        0   0    0   0     1    0     0     0     1          0   \n",
       "Nick McCaffery         0   0    0   0     0    0     0     0     0          0   \n",
       "David Perrone          0   0    0   0     0    0     0     0     0          0   \n",
       "Masoud                 1   0    0   0     0    0     0     0     0          0   \n",
       "Rony Lopes             0   0    0   0     0    0     0     0     0          0   \n",
       "Patrick Dowd           0   0    0   0     0    0     1     0     0          0   \n",
       "Ruifang Kuang          0   0    0   0     0    0     0     0     0          0   \n",
       "\n",
       "                   ...  this  to  today  very  wait  week  weeks  what  where  \\\n",
       "Demeus Alves       ...     0   0      0     0     0     0      0     0      0   \n",
       "Ryan Booth         ...     0   1      0     0     0     0      0     0      1   \n",
       "Brianna MacDonald  ...     0   0      0     0     0     0      0     0      0   \n",
       "Jair Delgado       ...     0   0      0     0     1     0      0     0      0   \n",
       "Shawn Vincent      ...     0   0      0     0     0     0      0     0      0   \n",
       "Jacob Afonso       ...     0   0      0     0     0     0      1     0      0   \n",
       "Ryan Buquicchio    ...     0   0      0     0     0     0      0     0      0   \n",
       "Nick McCaffery     ...     0   0      1     1     0     0      0     0      0   \n",
       "David Perrone      ...     1   0      0     0     0     0      0     0      0   \n",
       "Masoud             ...     0   0      0     0     0     0      0     1      0   \n",
       "Rony Lopes         ...     1   1      0     0     0     1      0     0      0   \n",
       "Patrick Dowd       ...     0   0      1     0     0     0      0     0      0   \n",
       "Ruifang Kuang      ...     0   0      0     0     0     0      0     0      0   \n",
       "\n",
       "                   windy  \n",
       "Demeus Alves           0  \n",
       "Ryan Booth             0  \n",
       "Brianna MacDonald      0  \n",
       "Jair Delgado           0  \n",
       "Shawn Vincent          0  \n",
       "Jacob Afonso           0  \n",
       "Ryan Buquicchio        0  \n",
       "Nick McCaffery         1  \n",
       "David Perrone          0  \n",
       "Masoud                 0  \n",
       "Rony Lopes             0  \n",
       "Patrick Dowd           0  \n",
       "Ruifang Kuang          0  \n",
       "\n",
       "[13 rows x 48 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.DataFrame(data=mat, index = text.keys(), columns=counts.get_feature_names() )\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the distances we use the `euclidean_distances` function.  To make this\n",
    "easy to read, we will put this in a dataframe as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Demeus Alves</th>\n",
       "      <th>Ryan Booth</th>\n",
       "      <th>Brianna MacDonald</th>\n",
       "      <th>Jair Delgado</th>\n",
       "      <th>Shawn Vincent</th>\n",
       "      <th>Jacob Afonso</th>\n",
       "      <th>Ryan Buquicchio</th>\n",
       "      <th>Nick McCaffery</th>\n",
       "      <th>David Perrone</th>\n",
       "      <th>Masoud</th>\n",
       "      <th>Rony Lopes</th>\n",
       "      <th>Patrick Dowd</th>\n",
       "      <th>Ruifang Kuang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Demeus Alves</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.645751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ryan Booth</th>\n",
       "      <td>3.872983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.741657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brianna MacDonald</th>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jair Delgado</th>\n",
       "      <td>3.316625</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>2.828427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shawn Vincent</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacob Afonso</th>\n",
       "      <td>3.162278</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>2.645751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ryan Buquicchio</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.872983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nick McCaffery</th>\n",
       "      <td>2.828427</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>2.236068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>David Perrone</th>\n",
       "      <td>2.449490</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>2.236068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Masoud</th>\n",
       "      <td>3.162278</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rony Lopes</th>\n",
       "      <td>3.316625</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>2.828427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patrick Dowd</th>\n",
       "      <td>2.828427</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.645751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ruifang Kuang</th>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Demeus Alves  Ryan Booth  Brianna MacDonald  Jair Delgado  \\\n",
       "Demeus Alves           0.000000    3.872983           2.645751      3.316625   \n",
       "Ryan Booth             3.872983    0.000000           3.741657      4.242641   \n",
       "Brianna MacDonald      2.645751    3.741657           0.000000      2.828427   \n",
       "Jair Delgado           3.316625    4.242641           2.828427      0.000000   \n",
       "Shawn Vincent          3.000000    4.000000           2.449490      2.828427   \n",
       "Jacob Afonso           3.162278    3.872983           2.645751      3.316625   \n",
       "Ryan Buquicchio        4.000000    4.582576           3.872983      4.358899   \n",
       "Nick McCaffery         2.828427    3.872983           2.236068      3.000000   \n",
       "David Perrone          2.449490    3.605551           2.236068      3.000000   \n",
       "Masoud                 3.162278    4.123106           2.645751      3.605551   \n",
       "Rony Lopes             3.316625    4.000000           2.828427      3.464102   \n",
       "Patrick Dowd           2.828427    3.605551           2.645751      3.316625   \n",
       "Ruifang Kuang          2.645751    3.741657           2.000000      2.828427   \n",
       "\n",
       "                   Shawn Vincent  Jacob Afonso  Ryan Buquicchio  \\\n",
       "Demeus Alves            3.000000      3.162278         4.000000   \n",
       "Ryan Booth              4.000000      3.872983         4.582576   \n",
       "Brianna MacDonald       2.449490      2.645751         3.872983   \n",
       "Jair Delgado            2.828427      3.316625         4.358899   \n",
       "Shawn Vincent           0.000000      3.000000         4.123106   \n",
       "Jacob Afonso            3.000000      0.000000         3.741657   \n",
       "Ryan Buquicchio         4.123106      3.741657         0.000000   \n",
       "Nick McCaffery          2.645751      2.828427         4.000000   \n",
       "David Perrone           2.645751      2.828427         2.828427   \n",
       "Masoud                  3.316625      3.464102         4.242641   \n",
       "Rony Lopes              3.162278      3.316625         4.358899   \n",
       "Patrick Dowd            3.000000      3.162278         4.000000   \n",
       "Ruifang Kuang           2.000000      2.645751         3.872983   \n",
       "\n",
       "                   Nick McCaffery  David Perrone    Masoud  Rony Lopes  \\\n",
       "Demeus Alves             2.828427       2.449490  3.162278    3.316625   \n",
       "Ryan Booth               3.872983       3.605551  4.123106    4.000000   \n",
       "Brianna MacDonald        2.236068       2.236068  2.645751    2.828427   \n",
       "Jair Delgado             3.000000       3.000000  3.605551    3.464102   \n",
       "Shawn Vincent            2.645751       2.645751  3.316625    3.162278   \n",
       "Jacob Afonso             2.828427       2.828427  3.464102    3.316625   \n",
       "Ryan Buquicchio          4.000000       2.828427  4.242641    4.358899   \n",
       "Nick McCaffery           0.000000       2.449490  3.162278    3.000000   \n",
       "David Perrone            2.449490       0.000000  2.828427    2.645751   \n",
       "Masoud                   3.162278       2.828427  0.000000    3.605551   \n",
       "Rony Lopes               3.000000       2.645751  3.605551    0.000000   \n",
       "Patrick Dowd             2.449490       2.449490  2.828427    3.316625   \n",
       "Ruifang Kuang            2.236068       2.236068  3.000000    2.828427   \n",
       "\n",
       "                   Patrick Dowd  Ruifang Kuang  \n",
       "Demeus Alves           2.828427       2.645751  \n",
       "Ryan Booth             3.605551       3.741657  \n",
       "Brianna MacDonald      2.645751       2.000000  \n",
       "Jair Delgado           3.316625       2.828427  \n",
       "Shawn Vincent          3.000000       2.000000  \n",
       "Jacob Afonso           3.162278       2.645751  \n",
       "Ryan Buquicchio        4.000000       3.872983  \n",
       "Nick McCaffery         2.449490       2.236068  \n",
       "David Perrone          2.449490       2.236068  \n",
       "Masoud                 2.828427       3.000000  \n",
       "Rony Lopes             3.316625       2.828427  \n",
       "Patrick Dowd           0.000000       2.645751  \n",
       "Ruifang Kuang          2.645751       0.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df = pd.DataFrame(data = euclidean_distances(text_df),\n",
    "                       index=  text.keys(), columns= text.keys())\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we find who's sentence was most similar to Masoud's?\n",
    "\n",
    "\n",
    "We can select his column and take the min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df['Masoud'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this will return zero, because it's the distance to the same sentence, so\n",
    "we can drop that row of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Demeus Alves         3.162278\n",
       "Ryan Booth           4.123106\n",
       "Brianna MacDonald    2.645751\n",
       "Jair Delgado         3.605551\n",
       "Shawn Vincent        3.316625\n",
       "Jacob Afonso         3.464102\n",
       "Ryan Buquicchio      4.242641\n",
       "Nick McCaffery       3.162278\n",
       "David Perrone        2.828427\n",
       "Rony Lopes           3.605551\n",
       "Patrick Dowd         2.828427\n",
       "Ruifang Kuang        3.000000\n",
       "Name: Masoud, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df['Masoud'].drop('Masoud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then min gives us the the value that's the minumum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6457513110645907"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df['Masoud'].drop('Masoud').min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use idx min instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brianna MacDonald'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df['Masoud'].drop('Masoud').idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it yourself\n",
    "\n",
    "1. Which two people wrote the most similar sentences?\n",
    "1. Using the feature space defined by the text above, what would the following\n",
    "sentence be as a vector?\n",
    "- \"Thanksgiving is a short week\"?\n",
    "- \"Rainy, windy days are cold\"\n",
    "1. What word was used the most in the whole set of sentences?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.6.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "source_map": [
   12,
   16,
   21,
   23,
   28,
   41,
   48,
   52,
   54,
   66,
   71,
   73,
   95,
   114,
   117,
   122,
   125,
   128,
   130,
   135,
   137,
   142,
   144,
   148,
   150,
   154,
   156,
   159,
   161,
   165,
   168,
   172,
   174,
   180,
   183,
   188,
   192,
   199,
   201,
   206,
   208,
   212,
   214,
   218,
   220
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}