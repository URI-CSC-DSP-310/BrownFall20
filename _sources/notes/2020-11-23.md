---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.12
    jupytext_version: 1.6.0
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Class 32: Intro to NLP

+++

1. say hello on zoom
1. share a sentence on the doc linked on prismia

```{code-cell} ipython3
import numpy as np
```

## Reviewing Confidence Intervals


```{code-cell} ipython3
# %load http://drsmb.co/310
def classification_confint(acc, n):
    '''
    Compute the 95% confidence interval for a classification problem.
      acc -- classification accuracy
      n   -- number of observations used to compute the accuracy
    Returns a tuple (lb,ub)
    '''
    interval = 1.96*np.sqrt(acc*(1-acc)/n)
    lb = max(0, acc - interval)
    ub = min(1.0, acc + interval)
    return (lb,ub)
```

If you trained to classifiers on the same data and evaluated on 50 test samples,
to get accuracies of 78% and 90% is the difference significant?

To check, we compute the confidence interval for each.

```{code-cell} ipython3
classification_confint(.78,50)
```

```{code-cell} ipython3
classification_confint(.9,50)
```

Then we check to see if the intervals overlap.  They do, so these are not significantly
different.

This means that while those seem meaningfully different, with 50 samples, 78% vs 50%
is not statistically significantly different. This means that we can't formally
guarantee that the two classifiers have reliablly different perforamnce.

If we had more samples, it could be, for example, for 200 samples we see that
they are different.

```{code-cell} ipython3
N =200
classification_confint(.9,N)
```

```{code-cell} ipython3
classification_confint(.78,N)
```

## Natural Language Processing


The first thing we need to do to be able to do to model text is transform to a
numerical representation.  We can't use any of the models we've seen so far,
or other models, on non numerical data.  

terms:

- document: unit of text we're analyzing (one sample)
- token:  sequence of characters in some particular document that are grouped together as a useful semantic unit for processing (basically a word)
- stop words: no meaning, we don't need them (like a, the, an,). Note that this is context dependent. [more info](https://scikit-learn.org/stable/modules/feature_extraction.html#using-stop-words)


### Representation

vector or bag of words, implemented by the `CountVectorizer`

Some sample text:

```{code-cell} ipython3
# %load http://drsmb.co/310
text = {
'Demeus Alves':'Hope everybody is staying safe',
'Ryan Booth':'The power is out where I live, might be forced to leave soon',
'Brianna MacDonald':'Rainy days',
'Jair Delgado':'Can not wait for lunch... hungry',
'Shawn Vincent':'I am excited for Thanksgiving',
'Jacob Afonso':'Short weeks are the best!',
'Ryan Buquicchio':'The sentence is sentence. (Best sentence ever)',
'Nick McCaffery':'Very windy today',
'David Perrone':'this is a sentence',
'Masoud':'It is rainy here. What about there?',
'Rony Lopes':'I get to relax later this week',
'Patrick Dowd':'It is cold out today',
'Ruifang Kuang':'Happy Thanksgiving!',
}
```

```{code-cell} ipython3
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import euclidean_distances
```

Let's try it on one:


```{code-cell} ipython3
s1 = text['Demeus Alves']
s1
```

first we initalize the object
```{code-cell} ipython3
counts = CountVectorizer()
```

Then we can fit and transform at once, this will build the representation and return
the input represented that way.

```{code-cell} ipython3
counts.fit_transform([s1])
```

It tells us the size and that it's a "sparse matrix" but that doesnt' display much more
To see more we can cast it to a regular array

```{code-cell} ipython3
counts.fit_transform([s1]).toarray()
```
This doesn't tell us much because this is all ones.

Or look at the "vocabulary" also called the "dictionary" for the whole representation
```{code-cell} ipython3
counts.vocabulary_
```

We can instead apply to the whole dataset.

```{code-cell} ipython3
counts.fit_transform(text.values())
```
Now there are more rows (samples/documents) and more columns (words in vocabulary)

```{code-cell} ipython3
counts.vocabulary_
```

We can save the transformed data to a variable

```{code-cell} ipython3
mat = counts.fit_transform(text.values()).toarray()
mat
```

To make it easier to read, we can use a dataFrame

```{code-cell} ipython3
import pandas as pd
```

The index is the keys of the dictionary of the sentences. The columns are the
words from the vocabulary. The  `get_feature_names` method will return them as a
sorted list instead of a dictionary with numbers.

```{code-cell} ipython3
text_df = pd.DataFrame(data=mat, index = text.keys(), columns=counts.get_feature_names() )
text_df
```

To compute the distances we use the `euclidean_distances` function.  To make this
easy to read, we will put this in a dataframe as well.

```{code-cell} ipython3
dist_df = pd.DataFrame(data = euclidean_distances(text_df),
                       index=  text.keys(), columns= text.keys())
dist_df
```

How can we find who's sentence was most similar to Masoud's?


We can select his column and take the min.

```{code-cell} ipython3
dist_df['Masoud'].min()
```

But this will return zero, because it's the distance to the same sentence, so
we can drop that row of the column

```{code-cell} ipython3
dist_df['Masoud'].drop('Masoud')
```

Then min gives us the the value that's the minumum.

```{code-cell} ipython3
dist_df['Masoud'].drop('Masoud').min()
```

We can use idx min instead.

```{code-cell} ipython3
dist_df['Masoud'].drop('Masoud').idxmin()
```


## Try it yourself

1. Which two people wrote the most similar sentences?
1. Using the feature space defined by the text above, what would the following
sentence be as a vector?
- "Thanksgiving is a short week"?
- "Rainy, windy days are cold"
1. What word was used the most in the whole set of sentences?
