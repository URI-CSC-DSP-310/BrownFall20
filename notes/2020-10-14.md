---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.12
    jupytext_version: 1.6.0
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Class 16: Naive Bayes Classification

+++

To learn a classifier, we need labeled data (features and target)

We split our data twice:
- sample-wise: test and train
- variable-wise: features and target

```{code-cell} ipython3
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
```

```{code-cell} ipython3
iris_df = sns.load_dataset('iris')
```

```{code-cell} ipython3
from sklearn.datasets import load_iris
```

```{code-cell} ipython3
iris_df = load_iris(as_frame=True, return_X_y = True)[0]
iris_df['species'] = load_iris(as_frame=True, return_X_y = True)[1]
```

```{code-cell} ipython3
iris_df.head()
```

```{code-cell} ipython3
sns.pairplot(data= iris_df, hue='species')
```

```{code-cell} ipython3
gnb = GaussianNB()
```

```{code-cell} ipython3
type(gnb)
```

```{code-cell} ipython3
iris_df.values[:,:4]
```

```{code-cell} ipython3
X_train, X_test,  y_train, y_test = train_test_split(iris_df.values[:,:4],
                                                     iris_df.values[:,-1],
                                                    test_size = .5,
                                                    random_state =0)
```

```{code-cell} ipython3
gnb.fit(X_train, y_train)
```

```{code-cell} ipython3
y_pred = gnb.predict(X_test)
y_pred
```

```{code-cell} ipython3
y_test
```

```{code-cell} ipython3
sum(y_pred == y_test)
```

```{code-cell} ipython3
len(y_pred)
```

```{code-cell} ipython3

```


## Questions after class



### I'm curious about other classifiers

Great! We'll see more Friday, next week, and next month.  You're also encouraged to try out others and experiment with sci-kit learn.

###  How could this be related to NLP?

Naive Bayes isn't a good classifier for NLP most of the time because word are not independent, but for very coarse distinctions it can be applied. The _Gaussian_ naive Bayes we used today doesn't apply to NLP without a lot of transformation because in general, words are more like categorical (or binary/dummies) variables than continuous valued. 

We'll work with text data toward the end of the semester.

## How we got the values from the DataFrame

A DataFrame is an object and one of the attributes is [`values`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html#pandas.DataFrame.values), we accessed that directly. However, there is a new, safer way through the [`to_numpy` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html#pandas.DataFrame.to_numpy).

<!-- Why is this algorithm fast? -->
