---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.12
    jupytext_version: 1.6.0
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Class 16: Naive Bayes Classification

+++

To learn a classifier, we need labeled data (features and target)

We split our data twice:
- sample-wise: test and train
- variable-wise: features and target

## Naive Bayes with Sci-kit Learn

We will use a new package today, [Scikit-Learn](https://scikit-learn.org/stable/index.html). Its package name for importing is `sklearn` but we don't import it with an alias, in general.  It's a large module and we most often import just the parts we need.  

````{margin}
```{tip}
Recall when a word turns green & bold in a notebook, it's a python keyword, or reserved word.
```
````

To do that we use a new Python keyword `from`.  We can identify a package and then import a submodule or a package and submodule with `.` and then import specific functions or classes.  


```{code-cell} ipython3
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
```

We can tell from this code that `test_train_split` is probably a function because it's in lowercase and `sklearn` follows [PEP 8](https://www.python.org/dev/peps/pep-0008/) the Python Style Guide pretty strictly.  We can also check with type

```{code-cell} ipython3
type(train_test_split)
```

We can tell [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) is probably a class because it's in [CapWords](https://www.python.org/dev/peps/pep-0008/#class-names), also known as [camel case](https://en.wikipedia.org/wiki/Camel_case).

Again we can check.

```{code-cell} ipython3
type(GaussianNB)
```

That's an abstract base class.

Today we'll work with the [iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset, which has been used for demonstrating statistical analyses since 1936.  It contains 4 measurements of flowers from 3 different species.

```{code-cell} ipython3
iris_df = sns.load_dataset('iris')
```
As usual, we look at the structure.

```{code-cell} ipython3
iris_df.head()
```

Next we examine the data litle further to consider what the structure is like for classification purposes.  

```{code-cell} ipython3
sns.pairplot(data= iris_df, hue='species')
```

In order for classification to work, we're looking to see that the groups of samples of different classes (here, species) do not overlap too much.  We're also looking at the shape of the groups of samples of each class.  

## Naive Bayes

Naive Bayes assumes that the features are uncorrelated (Naive) and that we can pick the most probable class.  It can assume different distributions of the features conditioned on the class, though.  We'll use Gaussian Naive Bayes.  Gaussian distributed samples in the plot above would be roughly ovals with more points at the center and uncorrelated Gaussian data would be distributed in circles, not ovals.  We have some ovals and some overlap, but enough we can expect this classifier to work pretty well.


First we instantiate the classifier object with the constructor method.
```{code-cell} ipython3
gnb = GaussianNB()
```

We just made it exist so far, nothing more, so we can check its type.

```{code-cell} ipython3
type(gnb)
```

We can also use the `get_params` method to see what it looks like.  

```{code-cell} ipython3
gnb.get_params()
```

## Training a Model


Before we trian the model, we have to get our data out of the DataFrame, because our `gnb.fit` method takes arrays. We can use the `values` attribute
```{code-cell} ipython3
iris_df.values
```

Then we create test an train splits of our data.  we'll to equal parts (`test_size = .5`) and set the random state so that we call get the same result
```{code-cell} ipython3
X_train, X_test,  y_train, y_test = train_test_split(iris_df.values[:,:4],
                                                     iris_df.values[:,-1],
                                                    test_size = .5,
                                                    random_state =0)
```

```{admonition} try it yourself!
1. rerun the test `train_test_split` without random state, to see how it's different
1. change the `test_size` to different sizes and see what happens.
```

Now we use the _training_ data to fit our model.
```{code-cell} ipython3
gnb.fit(X_train, y_train)
```

Now we can predict using the _test_ data's features only.

```{code-cell} ipython3
y_pred = gnb.predict(X_test)
y_pred
```

We can compare this to the y_test to see how well our classifier works.

```{code-cell} ipython3
y_test
```

We can simply use base python to get the number correct with a boolean and sum since `False` turns to 0 and `True` to 1.

```{code-cell} ipython3
sum(y_pred == y_test)
```

and compare that with the total
```{code-cell} ipython3
len(y_pred)
```

or compute accuracy
```{code-cell} ipython3
sum(y_pred == y_test)/len(y_pred)
```


## Questions after class



### I'm curious about other classifiers

Great! We'll see more Friday, next week, and next month.  You're also encouraged to try out others and experiment with sci-kit learn.

###  How could this be related to NLP?

Naive Bayes isn't a good classifier for NLP most of the time because word are not independent, but for very coarse distinctions it can be applied. The _Gaussian_ naive Bayes we used today doesn't apply to NLP without a lot of transformation because in general, words are more like categorical (or binary/dummies) variables than continuous valued.

We'll work with text data toward the end of the semester.

## How we got the values from the DataFrame

A DataFrame is an object and one of the attributes is [`values`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html#pandas.DataFrame.values), we accessed that directly. However, there is a new, safer way through the [`to_numpy` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html#pandas.DataFrame.to_numpy).

<!-- Why is this algorithm fast? -->
