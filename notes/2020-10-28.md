---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.12
    jupytext_version: 1.6.0
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Class 22: More Regression, More Evaluation and LASSO

1. log prismia chat
1. say hello in the zoom chat


```{code-cell} ipython3
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

# Load the diabetes dataset
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)
```

## Questions after class Monday

Some good questions were asked on the form Monday. Check the [notes](2020-10-26) for insight on the `r2_score` and `mean_square_error`.

## Review of Test Train splits


```{code-cell} ipython3
X_train,X_test, y_train,y_test = train_test_split(diabetes_X, diabetes_y ,
                                                  test_size=20,random_state=0)
```


## What metric does the `score` method of  `LinearRegression` use?

```{code-cell} ipython3
regr = linear_model.LinearRegression()
regr.fit(X_train, y_train)
```

```{code-cell} ipython3
regr.score(X_test,y_test)
```

```{code-cell} ipython3
y_pred = regr.predict(X_test)
r2_score(y_test,y_pred)
```

```{code-cell} ipython3
mean_squared_error(y_test,y_pred)
```

## Digging in deeper to the Linear Regression model

Linear regression fitting involvles learning the coefficients
```{code-cell} ipython3
regr.coef_
```
and an intercept
```{code-cell} ipython3
 regr.intercept_
```

The linear regression model is

$$ y = wx + b$$

wehere it stores $w$ in the `coef_` attribute and `b` as the `intercept_`

We can check how this works by multiplying one sample by the coefficients, to get a vector
```{code-cell} ipython3
X_test[0]*regr.coef_
```

then taking the sum and adding the intercept
```{code-cell} ipython3
np.sum(X_test[0]*regr.coef_) + regr.intercept_
```

and then comparing to the predictions.
```{code-cell} ipython3
y_pred[0]
```
These are not exactly the same due to float rounding erros, but they're very close.

````{margin}
```{tip}
by default `np.sum` sums across both axes so it gets a single value.  we want to only sum on 1 so that we get t
````
We can also check for the whole test set
```{code-cell} ipython3
errors = np.sum(X_test*regr.coef_,axis=1) + regr.intercept_ -y_pred
errors
```

and confirm these are very small
```{code-cell} ipython3
np.max(errors)
```

It's doing linear regression as we expected.


## Changing the complexity in Linear regression

It can be important to also know what the model is really doing and see how different features are used or not.  Maybe, for example all of the features are expensive to measure, but for testing we measured a lot of them.  We might want to simultaneously learn which features we actually need *and* the linear model.  

LASSO can do that for us it's objective is like linear regression, but it adds an extra term.  This term forces some of the learned coefficients to be 0.  Mulitplying by 0 gives 0, so that's like throwing away that feature. The term is called the $1-norm$, the details of the math are not important here, just the idea that it can reduce the number of features used as well.

$$ ||y-wx||_2^2 + \alpha ||w||_1 $$

```{code-cell} ipython3
lasso = linear_model.Lasso()
lasso.fit(X_train, y_train)
lasso.score(X_test,y_test)
```

It uses many fewer
```{code-cell} ipython3
lasso.coef_
```

It has a parameter alpha that must be >0 that we can use to control how many features it uses. When `alpha = 0 ` it's the same as linear regression, but the regular linear regression estimator uses a more numerically stable algorithm for the `fit` method

```{code-cell} ipython3
lasso2 = linear_model.Lasso(alpha=.5)
lasso2.fit(X_train, y_train)
lasso2.score(X_test,y_test)
```

We see tha fewer are 0 now.
```{code-cell} ipython3
lasso2.coef_
```

```{code-cell} ipython3
regr.score(X_train, y_train)
```

```{code-cell} ipython3
lasso2.score(X_train, y_train)
```

```{code-cell} ipython3
lasso_cv = cross_val_score(lasso2, diabetes_X, diabetes_y)
lasso_cv
```

```{code-cell} ipython3
regr_cv = cross_val_score(regr,diabetes_X, diabetes_y )
regr_cv
```

```{code-cell} ipython3
np.mean(lasso_cv), np.mean(regr_cv)
```

```{code-cell} ipython3
cross_val_score(regr,diabetes_X, diabetes_y ,cv=10)
```

We can do cross validation for all three of these models and look at both the mean and the standard deviation.  The mean tells us how well on average each model does on different subsets of the data.
The standard deviation is a measure of _spread_ of the scores.  For intuition, another measure of spread is `max(cv_scores) - min(cv_scores)`.

````{margin}
```{tip}
To keep things concise, we can put two values on one line. This creates a `tuple` and prints out both values since it's on the last line of the cell.
```
````


```{code-cell} ipython3
regr_cv = cross_val_score(regr,diabetes_X, diabetes_y ,cv=10)
np.mean(regr_cv), np.std(regr_cv)
```

```{code-cell} ipython3
lasso_cv = cross_val_score(lasso,diabetes_X, diabetes_y ,cv=10)
np.mean(lasso_cv), np.std(lasso_cv)
```

```{code-cell} ipython3
lasso2_cv = cross_val_score(lasso2,diabetes_X, diabetes_y ,cv=10)
np.mean(lasso2_cv), np.std(lasso2_cv)
```

## Questions after class

### What does the `cv` parameter in `cross_val_score` do?

First, let's look at the help.
```{code-cell} ipython3
help(cross_val_score)
```

It does different things, depending on what type of value we pass it.  We passed it an `int` (`10`), so what it did was split the data into 10 groups and then trains on 9/10 of those parts and tests on the last one. Then it iterates over those folds.

For classification, it can take into consideration the target value (classes) and an additionally specified group in the data.  A good visualization of what it does is shown in the [sklearn docs](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html).

It uses StratifiedKfold for classification, but since we're using regression it will use `KFold`. `test_train_split` uses `ShuffleSplit` by default, let's load that too to see what it does.

```{warning}
The key in the following is to get the _concepts_ not all of the details in how I evaluate and visualize.  I could have made figures separately to explain the concept, but I like to show that Python is self contained.
```


```{code-cell} ipython3
from sklearn.model_selection import KFold, ShuffleSplit
```

```{code-cell} ipython3
kf = KFold(n_splits = 10)
```

When we use the `split` method it gives us a generator.


```{code-cell} ipython3
kf.split(diabetes_X, diabetes_y)
```

We can use this in a loop to get the list of indices that will be used to get the test and train data for each fold.  To visualize what this is  doing, see below.

```{code-cell} ipython3
N_samples = len(diabetes_y)
kf_tt_df = pd.DataFrame(index=list(range(N_samples)))
i = 1
for train_idx, test_idx in kf.split(diabetes_X, diabetes_y):
    kf_tt_df['split ' + str(i)] = ['unused']*N_samples
    kf_tt_df['split ' + str(i)][train_idx] = 'Train'
    kf_tt_df['split ' + str(i)][test_idx] = 'Test'
    i +=1
```

```{margin}
How would you use those indices to get a out actual test and train data?
```

We can count how many times 'Test' and 'Train' appear
```{code-cell} ipython3
count_test = lambda part: len([v for v in part if v=='Test'])
count_train = lambda part: len([v for v in part if v=='Train'])
```

When we apply this along `axis=1` we to check that each sample is used in exactly 1 test set how may times each sample is used
```{code-cell} ipython3
sum(kf_tt_df.apply(count_test,axis = 1) ==1)
```


and exactly 9 training sets
```{code-cell} ipython3
sum(kf_tt_df.apply(count_test,axis = 1) ==9)
```

the describe helps ensure that all fo the values are exa

We can also visualize:
````{margin}
```{tip}
`sns.heatmap` doesn't work on strings, so we can replace them for the plotting
```
````

```{code-cell} ipython3
cmap = sns.color_palette("tab10",10)
g = sns.heatmap(kf_tt_df.replace({'Test':1,'Train':0}),cmap=cmap[7:9],cbar_kws={'ticks':[.25,.75]},linewidths=0,
    linecolor='gray')
colorbar = g.collections[0].colorbar
colorbar.set_ticklabels(['Train','Test'])
```
Note that unlike [`test_train_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) this does not always randomize and shuffle the data before splitting.

 If we apply those `lambda` functions along `axis=0`, we can see the size of each test set

```{code-cell} ipython3
kf_tt_df.apply(count_test,axis = 0)
```

and training set:
```{code-cell} ipython3
kf_tt_df.apply(count_train,axis = 0)
```

We can verify that these splits are the same size as what `test_train_split` does using the right settings.  10-fold splits the data into 10 parts and tests on 1, so that makes a test size of 1/10=.1, so we can use the `train_test_split` and check the length.

```
X_train2,X_test2, y_train2,y_test2 = train_test_split(diabetes_X, diabetes_y ,
                                                  test_size=.1,random_state=0)

[len(split) for split in [X_train2,X_test2,]]
```

Under the hood `train_test_split` uses `ShuffleSplit`
We can do a similar experiment as above to see what `ShuffleSplit` does.

```{code-cell} ipython3
skf = ShuffleSplit(10)
N_samples = len(diabetes_y)
ss_tt_df = pd.DataFrame(index=list(range(N_samples)))
i = 1
for train_idx, test_idx in skf.split(diabetes_X, diabetes_y):
    ss_tt_df['split ' + str(i)] = ['unused']*N_samples
    ss_tt_df['split ' + str(i)][train_idx] = 'Train'
    ss_tt_df['split ' + str(i)][test_idx] = 'Test'
    i +=1

ss_tt_df
```


And plot

```{code-cell} ipython3
cmap = sns.color_palette("tab10",10)
g = sns.heatmap(ss_tt_df.replace({'Test':1,'Train':0}),cmap=cmap[7:9],cbar_kws={'ticks':[.25,.75]},linewidths=0,
    linecolor='gray')
colorbar = g.collections[0].colorbar
colorbar.set_ticklabels(['Train','Test'])
```

Now, we see the samples in each training set (gray along the columns) are a random subset of all of all of the samples.

And check the usage of each sample
```{code-cell} ipython3
sum(ss_tt_df.apply(count_test,axis = 1) ==1)
```


and exactly 9 training sets
```{code-cell} ipython3
sum(ss_tt_df.apply(count_test,axis = 1) ==9)
```

And the size of the splits


```{code-cell} ipython3
ss_tt_df.apply(count_test,axis = 0)
```

and training set:
```{code-cell} ipython3
ss_tt_df.apply(count_train,axis = 0)
```

Again the same sizes
